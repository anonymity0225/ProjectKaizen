"""
Enterprise-grade Model Registry Service with comprehensive production features.

This module provides a complete model registry solution with:
- Two-phase commit transactions
- Distributed locking and concurrency control
- Multi-level caching and performance optimization
- Robust S3 operations with circuit breakers
- Semantic versioning and atomic rollbacks
- Blue-green deployments and traffic splitting
- Comprehensive monitoring and observability
- Audit logging and compliance features
- Disaster recovery and backup capabilities
"""

import os
import sys
import tempfile
import logging
import uuid
import time
import json
import hashlib
import pickle
import threading
import asyncio
import zstandard as zstd
from typing import Optional, Dict, Any, List, Tuple, Union, Callable, NamedTuple
from datetime import datetime, timedelta, timezone
from pathlib import Path
from dataclasses import dataclass, field
from enum import Enum, IntEnum
from contextlib import contextmanager, asynccontextmanager
from functools import wraps, lru_cache
from collections import defaultdict, deque
from threading import RLock, Event, Condition, Lock
from concurrent.futures import ThreadPoolExecutor, as_completed, Future
import weakref
import mmap

# Core dependencies
import pandas as pd
import numpy as np
import joblib
from sqlalchemy.orm import Session
from sqlalchemy.exc import SQLAlchemyError, OperationalError
from sqlalchemy import text, func, and_, or_
import redis
import boto3
from botocore.exceptions import ClientError, BotoCoreError

# Optional dependencies for monitoring and tracing
try:
    from prometheus_client import Counter, Histogram, Gauge, CollectorRegistry
    PROMETHEUS_AVAILABLE = True
except ImportError:
    PROMETHEUS_AVAILABLE = False

try:
    from opentelemetry import trace, metrics
    from opentelemetry.trace import Status, StatusCode
    OPENTELEMETRY_AVAILABLE = True
except ImportError:
    OPENTELEMETRY_AVAILABLE = False

# Internal imports
from app.models.registry import ModelRegistry, ModelPrediction
from app.database.connection import get_db_session
from app.storage.artifact_store import ArtifactStore, ArtifactStoreError
from app.schemas.modeling import (
    ModelConfig, TrainingResponse, EvaluationMetrics, 
    PredictionResponse, ModelInfo, ModelListResponse,
    ModelConfigExport, CleanupResult, ModelHealthStatus,
    ModelAlgorithm, ProblemType, ModelSummary
)
from app.config import settings

# Configure logging
logger = logging.getLogger(__name__)

# Initialize tracing and metrics if available
if OPENTELEMETRY_AVAILABLE:
    tracer = trace.get_tracer(__name__)
    meter = metrics.get_meter(__name__)
else:
    tracer = None
    meter = None

# Configuration from environment
CONFIG = {
    # Transaction settings
    'TRANSACTION_TIMEOUT_SECONDS': int(os.getenv('MODEL_REGISTRY_TRANSACTION_TIMEOUT', '300')),
    'LOCK_TIMEOUT_SECONDS': int(os.getenv('MODEL_REGISTRY_LOCK_TIMEOUT', '30')),
    'MAX_RETRY_ATTEMPTS': int(os.getenv('MODEL_REGISTRY_MAX_RETRIES', '3')),
    
    # Database connection pool
    'DB_POOL_MIN_SIZE': int(os.getenv('MODEL_REGISTRY_DB_POOL_MIN', '5')),
    'DB_POOL_MAX_SIZE': int(os.getenv('MODEL_REGISTRY_DB_POOL_MAX', '20')),
    'DB_POOL_RECYCLE_SECONDS': int(os.getenv('MODEL_REGISTRY_DB_POOL_RECYCLE', '3600')),
    
    # S3 settings
    'S3_RETRY_MAX_ATTEMPTS': int(os.getenv('MODEL_REGISTRY_S3_MAX_RETRIES', '5')),
    'S3_RETRY_INITIAL_DELAY': float(os.getenv('MODEL_REGISTRY_S3_INITIAL_DELAY', '1.0')),
    'S3_RETRY_MAX_DELAY': float(os.getenv('MODEL_REGISTRY_S3_MAX_DELAY', '60.0')),
    'S3_UPLOAD_BANDWIDTH_LIMIT': int(os.getenv('MODEL_REGISTRY_S3_UPLOAD_LIMIT', '52428800')),  # 50MB/s
    'S3_DOWNLOAD_BANDWIDTH_LIMIT': int(os.getenv('MODEL_REGISTRY_S3_DOWNLOAD_LIMIT', '104857600')),  # 100MB/s
    
    # Cache settings
    'CACHE_L1_SIZE': int(os.getenv('MODEL_REGISTRY_L1_CACHE_SIZE', '10')),
    'CACHE_L1_TTL_SECONDS': int(os.getenv('MODEL_REGISTRY_L1_TTL', '3600')),
    'CACHE_L2_TTL_SECONDS': int(os.getenv('MODEL_REGISTRY_L2_TTL', '86400')),
    'CACHE_L3_DISK_LIMIT_GB': int(os.getenv('MODEL_REGISTRY_L3_DISK_LIMIT', '50')),
    
    # Versioning
    'MAX_VERSIONS_PER_MODEL': int(os.getenv('MODEL_REGISTRY_MAX_VERSIONS', '5')),
    'ARCHIVE_DAYS': int(os.getenv('MODEL_REGISTRY_ARCHIVE_DAYS', '180')),
    
    # Redis settings
    'REDIS_HOST': os.getenv('MODEL_REGISTRY_REDIS_HOST', 'localhost'),
    'REDIS_PORT': int(os.getenv('MODEL_REGISTRY_REDIS_PORT', '6379')),
    'REDIS_DB': int(os.getenv('MODEL_REGISTRY_REDIS_DB', '1')),
    'REDIS_PASSWORD': os.getenv('MODEL_REGISTRY_REDIS_PASSWORD'),
}

# ==========================================
# ENUMS AND TYPE DEFINITIONS
# ==========================================

class TransactionState(Enum):
    """Transaction state enumeration."""
    INITIATED = "initiated"
    PHASE_1_COMPLETE = "phase_1_complete"
    COMMITTED = "committed"
    ROLLED_BACK = "rolled_back"
    FAILED = "failed"
    TIMED_OUT = "timed_out"

class ModelState(Enum):
    """Model lifecycle state."""
    STAGING = "staging"
    ACTIVE = "active"
    DEPRECATED = "deprecated"
    ARCHIVED = "archived"
    FAILED = "failed"

class DeploymentVariant(Enum):
    """Deployment variant types."""
    BLUE = "blue"
    GREEN = "green"
    CANARY = "canary"
    SHADOW = "shadow"

class CacheLevel(IntEnum):
    """Cache level enumeration."""
    L1_MEMORY = 1
    L2_REDIS = 2
    L3_DISK = 3

class OperationType(Enum):
    """Operation type for audit logging."""
    REGISTER = "register"
    UPDATE = "update"
    LOAD = "load"
    DELETE = "delete"
    ROLLBACK = "rollback"
    DEPLOY = "deploy"
    PREDICT = "predict"

class RiskLevel(Enum):
    """Model risk classification."""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

class CircuitBreakerState(Enum):
    """Circuit breaker state."""
    CLOSED = "closed"
    OPEN = "open"
    HALF_OPEN = "half_open"

# ==========================================
# EXCEPTIONS
# ==========================================

class ModelRegistryError(Exception):
    """Base exception for model registry errors."""
    pass

class ModelRegistrationError(ModelRegistryError):
    """Error during model registration."""
    pass

class ModelLoadError(ModelRegistryError):
    """Error during model loading."""
    pass

class ModelVersionConflictError(ModelRegistryError):
    """Version conflict error."""
    pass

class StorageError(ModelRegistryError):
    """Storage operation error."""
    pass

class CacheError(ModelRegistryError):
    """Cache operation error."""
    pass

class TransactionError(ModelRegistryError):
    """Transaction error."""
    pass

class LockAcquisitionError(ModelRegistryError):
    """Lock acquisition error."""
    pass

class CircuitBreakerOpenError(ModelRegistryError):
    """Circuit breaker is open."""
    pass

class ModelHealthCheckError(ModelRegistryError):
    """Model health check failed."""
    pass

class GovernanceError(ModelRegistryError):
    """Governance requirement not met."""
    pass

# ==========================================
# DATA CLASSES
# ==========================================

@dataclass
class TransactionLog:
    """Transaction log entry."""
    transaction_id: str
    operation_type: OperationType
    model_name: str
    version: str
    state: TransactionState
    started_at: datetime
    completed_at: Optional[datetime] = None
    error_message: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class ModelArtifact:
    """Model artifact information."""
    uri: str
    hash: str
    size: int
    format: str
    compression: Optional[str] = None
    checksum: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class CacheEntry:
    """Cache entry data."""
    key: str
    data: Any
    created_at: datetime
    expires_at: datetime
    access_count: int = 0
    last_accessed: Optional[datetime] = None
    size_bytes: int = 0

@dataclass
class ModelVersion:
    """Semantic version information."""
    major: int
    minor: int
    patch: int
    prerelease: Optional[str] = None
    build: Optional[str] = None
    
    def __str__(self) -> str:
        version = f"{self.major}.{self.minor}.{self.patch}"
        if self.prerelease:
            version += f"-{self.prerelease}"
        if self.build:
            version += f"+{self.build}"
        return version
    
    @classmethod
    def parse(cls, version_str: str) -> 'ModelVersion':
        """Parse version string into ModelVersion."""
        import re
        pattern = r'^(\d+)\.(\d+)\.(\d+)(?:-([^+]+))?(?:\+(.+))?$'
        match = re.match(pattern, version_str)
        if not match:
            raise ValueError(f"Invalid version format: {version_str}")
        
        major, minor, patch, prerelease, build = match.groups()
        return cls(
            major=int(major),
            minor=int(minor),
            patch=int(patch),
            prerelease=prerelease,
            build=build
        )

@dataclass
class DeploymentConfig:
    """Deployment configuration."""
    variant: DeploymentVariant
    traffic_percentage: float
    health_check_enabled: bool = True
    rollback_on_failure: bool = True
    canary_duration_minutes: int = 60
    success_threshold: float = 0.95
    error_threshold: float = 0.05

@dataclass
class ModelHealth:
    """Model health status."""
    is_healthy: bool
    load_time_ms: float
    prediction_latency_ms: float
    memory_usage_mb: float
    error_rate: float
    last_check: datetime
    issues: List[str] = field(default_factory=list)

@dataclass
class AuditEntry:
    """Audit log entry."""
    entry_id: str
    timestamp: datetime
    operation: OperationType
    model_id: Optional[str]
    model_name: Optional[str]
    version: Optional[str]
    user_id: Optional[str]
    session_id: Optional[str]
    ip_address: Optional[str]
    user_agent: Optional[str]
    details: Dict[str, Any]
    risk_score: int = 0
    signature: Optional[str] = None

# ==========================================
# UTILITY CLASSES
# ==========================================

class MetricsCollector:
    """Metrics collection for monitoring."""
    
    def __init__(self):
        self.registry = CollectorRegistry() if PROMETHEUS_AVAILABLE else None
        if PROMETHEUS_AVAILABLE:
            self.model_load_time = Histogram(
                'model_load_time_seconds',
                'Time taken to load a model',
                ['model_name', 'version', 'cache_level'],
                registry=self.registry
            )
            self.model_inference_time = Histogram(
                'model_inference_time_seconds',
                'Time taken for model inference',
                ['model_name', 'version'],
                registry=self.registry
            )
            self.cache_hit_rate = Counter(
                'cache_hits_total',
                'Number of cache hits',
                ['cache_level', 'hit_miss'],
                registry=self.registry
            )
            self.s3_operation_time = Histogram(
                's3_operation_time_seconds',
                'Time taken for S3 operations',
                ['operation', 'status'],
                registry=self.registry
            )
            self.active_models = Gauge(
                'active_models_total',
                'Number of active models',
                registry=self.registry
            )
            self.failed_operations = Counter(
                'failed_operations_total',
                'Number of failed operations',
                ['operation_type', 'error_type'],
                registry=self.registry
            )
    
    def record_model_load_time(self, model_name: str, version: str, cache_level: str, duration: float):
        """Record model load time."""
        if PROMETHEUS_AVAILABLE:
            self.model_load_time.labels(
                model_name=model_name,
                version=version,
                cache_level=cache_level
            ).observe(duration)
    
    def record_cache_hit(self, cache_level: str, hit: bool):
        """Record cache hit/miss."""
        if PROMETHEUS_AVAILABLE:
            self.cache_hit_rate.labels(
                cache_level=cache_level,
                hit_miss='hit' if hit else 'miss'
            ).inc()
    
    def record_s3_operation(self, operation: str, status: str, duration: float):
        """Record S3 operation metrics."""
        if PROMETHEUS_AVAILABLE:
            self.s3_operation_time.labels(
                operation=operation,
                status=status
            ).observe(duration)
    
    def set_active_models(self, count: int):
        """Set active models count."""
        if PROMETHEUS_AVAILABLE:
            self.active_models.set(count)
    
    def record_failed_operation(self, operation_type: str, error_type: str):
        """Record failed operation."""
        if PROMETHEUS_AVAILABLE:
            self.failed_operations.labels(
                operation_type=operation_type,
                error_type=error_type
            ).inc()

class CircuitBreaker:
    """Circuit breaker for fault tolerance."""
    
    def __init__(self, failure_threshold: int = 10, recovery_timeout: int = 30, success_threshold: int = 5):
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.success_threshold = success_threshold
        self.failure_count = 0
        self.success_count = 0
        self.last_failure_time = None
        self.state = CircuitBreakerState.CLOSED
        self._lock = RLock()
    
    def __call__(self, func):
        """Decorator to apply circuit breaker to a function."""
        @wraps(func)
        def wrapper(*args, **kwargs):
            with self._lock:
                if self.state == CircuitBreakerState.OPEN:
                    if self._should_attempt_reset():
                        self.state = CircuitBreakerState.HALF_OPEN
                    else:
                        raise CircuitBreakerOpenError("Circuit breaker is open")
                
                try:
                    result = func(*args, **kwargs)
                    self._on_success()
                    return result
                except Exception as e:
                    self._on_failure()
                    raise
        return wrapper
    
    def _should_attempt_reset(self) -> bool:
        """Check if we should attempt to reset the circuit breaker."""
        return (
            self.last_failure_time and
            time.time() - self.last_failure_time >= self.recovery_timeout
        )
    
    def _on_success(self):
        """Handle successful operation."""
        if self.state == CircuitBreakerState.HALF_OPEN:
            self.success_count += 1
            if self.success_count >= self.success_threshold:
                self.state = CircuitBreakerState.CLOSED
                self.failure_count = 0
                self.success_count = 0
        elif self.state == CircuitBreakerState.CLOSED:
            self.failure_count = 0
    
    def _on_failure(self):
        """Handle failed operation."""
        self.failure_count += 1
        self.last_failure_time = time.time()
        
        if self.failure_count >= self.failure_threshold:
            self.state = CircuitBreakerState.OPEN
            self.success_count = 0

class RedisLock:
    """Redis-based distributed lock using Redlock algorithm."""
    
    def __init__(self, redis_client: redis.Redis, key: str, timeout: int = 10):
        self.redis_client = redis_client
        self.key = f"lock:{key}"
        self.timeout = timeout
        self.identifier = str(uuid.uuid4())
        self.acquired = False
    
    def acquire(self) -> bool:
        """Acquire the lock."""
        end_time = time.time() + self.timeout
        while time.time() < end_time:
            if self.redis_client.set(self.key, self.identifier, nx=True, ex=self.timeout):
                self.acquired = True
                return True
            time.sleep(0.001)  # 1ms sleep
        return False
    
    def release(self) -> bool:
        """Release the lock."""
        if not self.acquired:
            return False
        
        # Lua script to atomically check and delete
        script = """
        if redis.call('get', KEYS[1]) == ARGV[1] then
            return redis.call('del', KEYS[1])
        else
            return 0
        end
        """
        result = self.redis_client.eval(script, 1, self.key, self.identifier)
        self.acquired = False
        return bool(result)
    
    def __enter__(self):
        if not self.acquire():
            raise LockAcquisitionError(f"Failed to acquire lock: {self.key}")
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.release()

class RetryWithBackoff:
    """Retry with exponential backoff."""
    
    def __init__(self, max_attempts: int = 3, initial_delay: float = 1.0, max_delay: float = 60.0, backoff_factor: float = 2.0):
        self.max_attempts = max_attempts
        self.initial_delay = initial_delay
        self.max_delay = max_delay
        self.backoff_factor = backoff_factor
    
    def __call__(self, func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            delay = self.initial_delay
            
            for attempt in range(1, self.max_attempts + 1):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    if attempt == self.max_attempts:
                        break
                    
                    logger.warning(f"Attempt {attempt} failed: {e}. Retrying in {delay}s...")
                    time.sleep(delay)
                    delay = min(delay * self.backoff_factor, self.max_delay)
            
            raise last_exception
        return wrapper

# Global instances
metrics_collector = MetricsCollector()
s3_circuit_breaker = CircuitBreaker(failure_threshold=10, recovery_timeout=30)
db_circuit_breaker = CircuitBreaker(failure_threshold=5, recovery_timeout=60)

# ==========================================
# DISTRIBUTED LOCKING MANAGER
# ==========================================

class DistributedLockManager:
    """Manages distributed locks using Redis."""
    
    def __init__(self, redis_host: str = None, redis_port: int = None, redis_password: str = None):
        self.redis_client = None
        self._initialize_redis(redis_host, redis_port, redis_password)
    
    def _initialize_redis(self, host: str = None, port: int = None, password: str = None):
        """Initialize Redis client."""
        try:
            self.redis_client = redis.Redis(
                host=host or CONFIG['REDIS_HOST'],
                port=port or CONFIG['REDIS_PORT'],
                db=CONFIG['REDIS_DB'],
                password=password or CONFIG['REDIS_PASSWORD'],
                decode_responses=True,
                socket_timeout=5,
                socket_connect_timeout=5
            )
            # Test connection
            self.redis_client.ping()
            logger.info("Redis distributed lock manager initialized")
        except Exception as e:
            logger.warning(f"Failed to initialize Redis for distributed locking: {e}")
            self.redis_client = None
    
    def acquire_lock(self, key: str, timeout: int = None) -> RedisLock:
        """Acquire a distributed lock."""
        if not self.redis_client:
            raise LockAcquisitionError("Redis not available for distributed locking")
        
        timeout = timeout or CONFIG['LOCK_TIMEOUT_SECONDS']
        return RedisLock(self.redis_client, key, timeout)

# Global lock manager
lock_manager = DistributedLockManager()

# ==========================================
# TWO-PHASE COMMIT TRANSACTION MANAGER
# ==========================================

class ModelRegistrationTransaction:
    """Two-phase commit transaction for model registration."""
    
    def __init__(self, transaction_id: str, model_name: str, version: str, artifact_store: ArtifactStore):
        self.transaction_id = transaction_id
        self.model_name = model_name
        self.version = version
        self.artifact_store = artifact_store
        self.state = TransactionState.INITIATED
        self.started_at = datetime.now(timezone.utc)
        self.staging_artifacts: List[str] = []
        self.rollback_actions: List[Callable] = []
        self.timeout_seconds = CONFIG['TRANSACTION_TIMEOUT_SECONDS']
        self._lock = RLock()
        self.metadata: Dict[str, Any] = {}
    
    def phase_1_upload_artifacts(self, model_data: bytes, explanations_data: Optional[bytes] = None) -> bool:
        """Phase 1: Upload artifacts to staging with retry logic."""
        try:
            with self._lock:
                if self.state != TransactionState.INITIATED:
                    raise TransactionError(f"Invalid state for phase 1: {self.state}")
                
                # Upload model with staging prefix
                staging_model_uri = f"staging/{self.transaction_id}/model.joblib.zst"
                compressed_model = zstd.compress(model_data, level=3)
                
                @RetryWithBackoff(max_attempts=CONFIG['MAX_RETRY_ATTEMPTS'])
                @s3_circuit_breaker
                def upload_model():
                    return self.artifact_store.upload_bytes(
                        data=compressed_model,
                        key=staging_model_uri,
                        metadata={
                            'transaction_id': self.transaction_id,
                            'status': 'staging',
                            'compression': 'zstd-3'
                        }
                    )
                
                model_result = upload_model()
                self.staging_artifacts.append(staging_model_uri)
                self.metadata['model_uri'] = staging_model_uri
                self.metadata['model_hash'] = model_result['hash']
                self.metadata['model_size'] = len(compressed_model)
                
                # Upload explanations if provided
                if explanations_data:
                    staging_exp_uri = f"staging/{self.transaction_id}/explanations.joblib.zst"
                    compressed_exp = zstd.compress(explanations_data, level=3)
                    
                    @RetryWithBackoff(max_attempts=CONFIG['MAX_RETRY_ATTEMPTS'])
                    @s3_circuit_breaker
                    def upload_explanations():
                        return self.artifact_store.upload_bytes(
                            data=compressed_exp,
                            key=staging_exp_uri,
                            metadata={
                                'transaction_id': self.transaction_id,
                                'status': 'staging',
                                'compression': 'zstd-3'
                            }
                        )
                    
                    exp_result = upload_explanations()
                    self.staging_artifacts.append(staging_exp_uri)
                    self.metadata['explanations_uri'] = staging_exp_uri
                    self.metadata['explanations_hash'] = exp_result['hash']
                
                self.state = TransactionState.PHASE_1_COMPLETE
                logger.info(f"Transaction {self.transaction_id} phase 1 complete")
                return True
                
        except Exception as e:
            logger.error(f"Transaction {self.transaction_id} phase 1 failed: {e}")
            self.state = TransactionState.FAILED
            self._cleanup_staging_artifacts()
            raise TransactionError(f"Phase 1 failed: {e}")
    
    def phase_2_commit_database(self, model_record_data: Dict[str, Any]) -> bool:
        """Phase 2: Create database record and move artifacts to production."""
        try:
            with self._lock:
                if self.state != TransactionState.PHASE_1_COMPLETE:
                    raise TransactionError(f"Invalid state for phase 2: {self.state}")
                
                # Acquire distributed lock for model registration
                lock_key = f"model_registration:{self.model_name}"  
                with lock_manager.acquire_lock(lock_key):
                    
                    # Move artifacts from staging to production
                    production_uris = self._promote_artifacts_to_production()
                    
                    try:
                        # Create database record with row-level locking
                        with get_db_session() as session:
                            # Lock existing model records for update
                            session.execute(
                                text("SELECT 1 FROM model_registry WHERE name = :name FOR UPDATE"),
                                {"name": self.model_name}
                            )
                            
                            # Mark previous versions as not latest
                            session.query(ModelRegistry).filter(
                                ModelRegistry.name == self.model_name,
                                ModelRegistry.is_latest == True
                            ).update({'is_latest': False, 'version': func.now()})
                            
                            # Create new model record
                            model_record = ModelRegistry(
                                id=str(uuid.uuid4()),
                                name=self.model_name,
                                version=self.version,
                                artifact_uri=production_uris['model'],
                                artifact_hash=self.metadata['model_hash'],
                                artifact_size=self.metadata['model_size'],
                                explanation_uri=production_uris.get('explanations'),
                                is_latest=True,
                                state=ModelState.ACTIVE.value,
                                **model_record_data
                            )
                            
                            session.add(model_record)
                            session.commit()
                            
                            self.metadata['model_id'] = model_record.id
                            self.state = TransactionState.COMMITTED
                            
                            logger.info(f"Transaction {self.transaction_id} committed successfully")
                            return True
                            
                    except Exception as db_error:
                        # Rollback production artifacts on database failure
                        self._rollback_production_artifacts(production_uris)
                        raise db_error
                        
        except Exception as e:
            logger.error(f"Transaction {self.transaction_id} phase 2 failed: {e}")
            self.state = TransactionState.FAILED
            self._cleanup_staging_artifacts()
            raise TransactionError(f"Phase 2 failed: {e}")
    
    def rollback(self) -> bool:
        """Rollback the entire transaction."""
        try:
            with self._lock:
                logger.info(f"Rolling back transaction {self.transaction_id}")
                
                # Execute rollback actions in reverse order
                for action in reversed(self.rollback_actions):
                    try:
                        action()
                    except Exception as e:
                        logger.warning(f"Rollback action failed: {e}")
                
                # Clean up staging artifacts
                self._cleanup_staging_artifacts()
                
                self.state = TransactionState.ROLLED_BACK
                return True
                
        except Exception as e:
            logger.error(f"Rollback failed for transaction {self.transaction_id}: {e}")
            self.state = TransactionState.FAILED
            return False
    
    def _promote_artifacts_to_production(self) -> Dict[str, str]:
        """Move artifacts from staging to production location."""
        production_uris = {}
        
        for staging_uri in self.staging_artifacts:
            if 'model.joblib' in staging_uri:
                production_uri = f"models/{self.model_name}/{self.version}/model.joblib.zst"
                production_uris['model'] = production_uri
            elif 'explanations.joblib' in staging_uri:
                production_uri = f"models/{self.model_name}/{self.version}/explanations.joblib.zst"
                production_uris['explanations'] = production_uri
            
            # Copy from staging to production
            self.artifact_store.copy_object(staging_uri, production_uri)
            
            # Add rollback action
            self.rollback_actions.append(
                lambda uri=production_uri: self.artifact_store.delete_object(uri)
            )
        
        return production_uris
    
    def _rollback_production_artifacts(self, production_uris: Dict[str, str]):
        """Remove production artifacts on rollback."""
        for uri in production_uris.values():
            try:
                self.artifact_store.delete_object(uri)
            except Exception as e:
                logger.warning(f"Failed to rollback production artifact {uri}: {e}")
    
    def _cleanup_staging_artifacts(self):
        """Clean up staging artifacts."""
        for uri in self.staging_artifacts:
            try:
                self.artifact_store.delete_object(uri)
            except Exception as e:
                logger.warning(f"Failed to cleanup staging artifact {uri}: {e}")
    
    def is_expired(self) -> bool:
        """Check if transaction has expired."""
        return (datetime.now(timezone.utc) - self.started_at).total_seconds() > self.timeout_seconds

# ==========================================
# MULTI-LEVEL CACHE SYSTEM
# ==========================================

class MultiLevelCache:
    """Multi-level caching system for model registry."""
    
    def __init__(self):
        # Level 1: In-memory LRU cache
        self.l1_cache: Dict[str, CacheEntry] = {}
        self.l1_access_order: deque = deque()
        self.l1_max_size = CONFIG['CACHE_L1_SIZE']
        self.l1_ttl = timedelta(seconds=CONFIG['CACHE_L1_TTL_SECONDS'])
        self.l1_lock = RLock()
        
        # Level 2: Redis cache
        self.l2_client = None
        self.l2_ttl = CONFIG['CACHE_L2_TTL_SECONDS']
        self._initialize_redis()
        
        # Level 3: Disk cache
        self.l3_cache_dir = Path(tempfile.gettempdir()) / "model_registry_cache"
        self.l3_cache_dir.mkdir(exist_ok=True)
        self.l3_max_size_bytes = CONFIG['CACHE_L3_DISK_LIMIT_GB'] * 1024 * 1024 * 1024
        
        # Cache statistics
        self.stats = {
            'l1_hits': 0, 'l1_misses': 0,
            'l2_hits': 0, 'l2_misses': 0,
            'l3_hits': 0, 'l3_misses': 0
        }
    
    def _initialize_redis(self):
        """Initialize Redis client for L2 cache."""
        try:
            self.l2_client = redis.Redis(
                host=CONFIG['REDIS_HOST'],
                port=CONFIG['REDIS_PORT'],
                db=CONFIG['REDIS_DB'],
                password=CONFIG['REDIS_PASSWORD'],
                socket_timeout=5
            )
            self.l2_client.ping()
        except Exception as e:
            logger.warning(f"Redis L2 cache not available: {e}")
            self.l2_client = None
    
    def get(self, key: str) -> Optional[Any]:
        """Get item from cache, checking all levels."""
        # Try L1 cache first
        with self.l1_lock:
            if key in self.l1_cache:
                entry = self.l1_cache[key]
                if entry.expires_at > datetime.now(timezone.utc):
                    entry.access_count += 1
                    entry.last_accessed = datetime.now(timezone.utc)
                    self._update_l1_access_order(key)
                    self.stats['l1_hits'] += 1
                    metrics_collector.record_cache_hit('l1', True)
                    return entry.data
                else:
                    # Expired entry
                    del self.l1_cache[key]
                    self.l1_access_order.remove(key)
        
        self.stats['l1_misses'] += 1
        metrics_collector.record_cache_hit('l1', False)
        
        # Try L2 cache (Redis)
        if self.l2_client:
            try:
                cached_data = self.l2_client.get(f"model_cache:{key}")
                if cached_data:
                    data = pickle.loads(cached_data)
                    # Promote to L1 cache
                    self._set_l1(key, data, self.l1_ttl)
                    self.stats['l2_hits'] += 1
                    metrics_collector.record_cache_hit('l2', True)
                    return data
            except Exception as e:
                logger.warning(f"L2 cache error: {e}")
        
        self.stats['l2_misses'] += 1
        metrics_collector.record_cache_hit('l2', False)
        
        # Try L3 cache (Disk)
        l3_path = self.l3_cache_dir / f"{hashlib.md5(key.encode()).hexdigest()}.cache"
        if l3_path.exists():
            try:
                with open(l3_path, 'rb') as f:
                    cache_data = pickle.load(f)
                    if cache_data['expires_at'] > datetime.now(timezone.utc):
                        data = cache_data['data']
                        # Promote to higher levels
                        self._set_l1(key, data, self.l1_ttl)
                        if self.l2_client:
                            self._set_l2(key, data)
                        self.stats['l3_hits'] += 1
                        metrics_collector.record_cache_hit('l3', True)
                        return data
                    else:
                        l3_path.unlink()  # Remove expired cache
            except Exception as e:
                logger.warning(f"L3 cache error: {e}")
        
        self.stats['l3_misses'] += 1
        metrics_collector.record_cache_hit('l3', False)
        return None
    
    def set(self, key: str, data: Any, ttl: Optional[timedelta] = None):
        """Set item in all cache levels."""
        ttl = ttl or self.l1_ttl
        
        # Set in L1
        self._set_l1(key, data, ttl)
        
        # Set in L2 (Redis)
        if self.l2_client:
            self._set_l2(key, data)
        
        # Set in L3 (Disk) for larger items
        data_size = sys.getsizeof(data)
        if data_size > 1024 * 1024:  # Cache to disk if > 1MB
            self._set_l3(key, data, ttl)
    
    def _set_l1(self, key: str, data: Any, ttl: timedelta):
        """Set item in L1 cache."""
        with self.l1_lock:
            # Evict if necessary
            while len(self.l1_cache) >= self.l1_max_size:
                oldest_key = self.l1_access_order.popleft()
                del self.l1_cache[oldest_key]
            
            expires_at = datetime.now(timezone.utc) + ttl
            entry = CacheEntry(
                key=key,
                data=data,
                created_at=datetime.now(timezone.utc),
                expires_at=expires_at,
                size_bytes=sys.getsizeof(data)
            )
            
            self.l1_cache[key] = entry
            self.l1_access_order.append(key)
    
    def _set_l2(self, key: str, data: Any):
        """Set item in L2 cache."""
        try:
            serialized_data = pickle.dumps(data)
            self.l2_client.setex(
                f"model_cache:{key}",
                self.l2_ttl,
                serialized_data
            )
        except Exception as e:
            logger.warning(f"Failed to set L2 cache for {key}: {e}")
    
    def _set_l3(self, key: str, data: Any, ttl: timedelta):
        """Set item in L3 cache."""
        try:
            l3_path = self.l3_cache_dir / f"{hashlib.md5(key.encode()).hexdigest()}.cache"
            cache_data = {
                'data': data,
                'created_at': datetime.now(timezone.utc),
                'expires_at': datetime.now(timezone.utc) + ttl
            }
            
            with open(l3_path, 'wb') as f:
                pickle.dump(cache_data, f)
            
            # Clean up old cache files if necessary
            self._cleanup_l3_cache()
            
        except Exception as e:
            logger.warning(f"Failed to set L3 cache for {key}: {e}")
    
    def _update_l1_access_order(self, key: str):
        """Update access order for LRU eviction."""
        self.l1_access_order.remove(key)
        self.l1_access_order.append(key)
    
    def _cleanup_l3_cache(self):
        """Clean up L3 cache to stay within size limits."""
        try:
            total_size = sum(f.stat().st_size for f in self.l3_cache_dir.glob('*.cache'))
            
            if total_size > self.l3_max_size_bytes:
                # Sort by access time and remove oldest
                cache_files = list(self.l3_cache_dir.glob('*.cache'))
                cache_files.sort(key=lambda f: f.stat().st_mtime)
                
                for cache_file in cache_files:
                    cache_file.unlink()
                    total_size -= cache_file.stat().st_size
                    if total_size <= self.l3_max_size_bytes * 0.8:  # Leave some headroom
                        break
        except Exception as e:
            logger.warning(f"L3 cache cleanup failed: {e}")
    
    def invalidate(self, key: str):
        """Invalidate item from all cache levels."""
        # L1 cache
        with self.l1_lock:
            if key in self.l1_cache:
                del self.l1_cache[key]
                self.l1_access_order.remove(key)
        
        # L2 cache
        if self.l2_client:
            try:
                self.l2_client.delete(f"model_cache:{key}")
            except Exception as e:
                logger.warning(f"Failed to invalidate L2 cache for {key}: {e}")
        
        # L3 cache
        l3_path = self.l3_cache_dir / f"{hashlib.md5(key.encode()).hexdigest()}.cache"
        if l3_path.exists():
            try:
                l3_path.unlink()
            except Exception as e:
                logger.warning(f"Failed to invalidate L3 cache for {key}: {e}")
    
    def get_stats(self) -> Dict[str, Any]:
        """Get cache statistics."""
        l1_size = len(self.l1_cache)
        l3_size = len(list(self.l3_cache_dir.glob('*.cache')))
        
        total_requests = sum([
            self.stats['l1_hits'], self.stats['l1_misses'],
            self.stats['l2_hits'], self.stats['l2_misses'],
            self.stats['l3_hits'], self.stats['l3_misses']
        ])
        
        overall_hit_rate = 0
        if total_requests > 0:
            total_hits = self.stats['l1_hits'] + self.stats['l2_hits'] + self.stats['l3_hits']
            overall_hit_rate = total_hits / total_requests
        
        return {
            'l1_cache_size': l1_size,
            'l3_cache_size': l3_size,
            'overall_hit_rate': overall_hit_rate,
            **self.stats
        }

# Global cache instance
global_cache = MultiLevelCache()

# ==========================================
# SEMANTIC VERSIONING MANAGER
# ==========================================

class SemanticVersionManager:
    """Manages semantic versioning for models."""
    
    def __init__(self):
        self.version_history: Dict[str, List[ModelVersion]] = {}
        self._lock = RLock()
    
    def validate_version(self, version_str: str) -> bool:
        """Validate semantic version format."""
        try:
            ModelVersion.parse(version_str)
            return True
        except ValueError:
            return False
    
    def detect_breaking_changes(self, old_metadata: Dict[str, Any], new_metadata: Dict[str, Any]) -> List[str]:
        """Detect breaking changes between model versions."""
        breaking_changes = []
        
        # Check feature column changes
        old_features = set(old_metadata.get('feature_columns', []))
        new_features = set(new_metadata.get('feature_columns', []))
        
        if old_features != new_features:
            removed_features = old_features - new_features
            added_features = new_features - old_features
            
            if removed_features:
                breaking_changes.append(f"Removed features: {list(removed_features)}")
            if added_features:
                breaking_changes.append(f"Added features: {list(added_features)}")
        
        # Check target variable changes
        if old_metadata.get('target_column') != new_metadata.get('target_column'):
            breaking_changes.append("Target column changed")
        
        # Check algorithm changes
        if old_metadata.get('algorithm') != new_metadata.get('algorithm'):
            breaking_changes.append("Algorithm changed")
        
        # Check problem type changes
        if old_metadata.get('problem_type') != new_metadata.get('problem_type'):
            breaking_changes.append("Problem type changed")
        
        return breaking_changes
    
    def suggest_next_version(self, model_name: str, breaking_changes: List[str], 
                           current_version: Optional[str] = None) -> str:
        """Suggest next version based on changes."""
        if not current_version:
            # Get latest version from database
            with get_db_session() as session:
                latest = session.query(ModelRegistry.version).filter(
                    ModelRegistry.name == model_name,
                    ModelRegistry.is_latest == True
                ).scalar()
                current_version = latest or "0.0.0"
        
        current = ModelVersion.parse(current_version)
        
        if breaking_changes:
            # Major version bump for breaking changes
            return str(ModelVersion(current.major + 1, 0, 0))
        else:
            # Minor version bump for non-breaking changes
            return str(ModelVersion(current.major, current.minor + 1, 0))
    
    def is_compatible(self, version1: str, version2: str) -> bool:
        """Check if two versions are compatible (same major version)."""
        v1 = ModelVersion.parse(version1)
        v2 = ModelVersion.parse(version2)
        return v1.major == v2.major
    
    def get_version_history(self, model_name: str) -> List[Dict[str, Any]]:
        """Get version history for a model."""
        with get_db_session() as session:
            versions = session.query(ModelRegistry).filter(
                ModelRegistry.name == model_name,
                ModelRegistry.is_active == True
            ).order_by(ModelRegistry.created_at.desc()).all()
            
            return [
                {
                    'version': v.version,
                    'created_at': v.created_at,
                    'is_latest': v.is_latest,
                    'metrics': v.metrics,
                    'state': v.state
                }
                for v in versions
            ]

# Global version manager
version_manager = SemanticVersionManager()

# ==========================================
# DISASTER RECOVERY MANAGER
# ==========================================

class DisasterRecoveryManager:
    """Handles backup, restore, and failover operations."""
    
    def __init__(self, artifact_store: ArtifactStore):
        self.artifact_store = artifact_store
        self.backup_bucket = os.getenv('MODEL_REGISTRY_BACKUP_BUCKET', 'model-registry-backup')
        self.cross_region_bucket = os.getenv('MODEL_REGISTRY_CROSS_REGION_BUCKET', 'model-registry-dr')
        self._lock = RLock()
    
    def create_full_backup(self, backup_id: str = None) -> Dict[str, Any]:
        """Create complete backup of registry database and artifacts."""
        backup_id = backup_id or f"backup_{int(time.time())}"
        backup_start = datetime.now(timezone.utc)
        
        try:
            # Backup database
            db_backup_path = self._backup_database(backup_id)
            
            # Backup all artifacts
            artifact_backup_paths = self._backup_artifacts(backup_id)
            
            # Create backup manifest
            manifest = {
                'backup_id': backup_id,
                'timestamp': backup_start.isoformat(),
                'database_backup': db_backup_path,
                'artifact_backups': artifact_backup_paths,
                'backup_size_mb': self._calculate_backup_size(db_backup_path, artifact_backup_paths),
                'status': 'completed'
            }
            
            # Store manifest
            manifest_path = f"backups/{backup_id}/manifest.json"
            self.artifact_store.upload_bytes(
                json.dumps(manifest).encode(),
                manifest_path,
                bucket=self.backup_bucket
            )
            
            # Cross-region replication
            self._replicate_to_cross_region(backup_id, manifest)
            
            duration = (datetime.now(timezone.utc) - backup_start).total_seconds()
            logger.info(f"Full backup completed: {backup_id} in {duration:.2f}s")
            
            return manifest
            
        except Exception as e:
            logger.error(f"Backup failed: {e}")
            raise
    
    def restore_from_backup(self, backup_id: str, target_region: str = None) -> bool:
        """Restore registry from backup."""
        try:
            # Load backup manifest
            manifest_path = f"backups/{backup_id}/manifest.json"
            bucket = self.cross_region_bucket if target_region else self.backup_bucket
            
            manifest_data = self.artifact_store.download_bytes(manifest_path, bucket=bucket)
            manifest = json.loads(manifest_data.decode())
            
            # Restore database
            self._restore_database(manifest['database_backup'], bucket)
            
            # Restore artifacts
            self._restore_artifacts(manifest['artifact_backups'], bucket)
            
            logger.info(f"Restore completed from backup: {backup_id}")
            return True
            
        except Exception as e:
            logger.error(f"Restore failed: {e}")
            return False
    
    def _backup_database(self, backup_id: str) -> str:
        """Backup database to file."""
        backup_path = f"backups/{backup_id}/database.sql"
        
        # Export database (implementation depends on DB type)
        sql_dump = self._export_database()
        
        self.artifact_store.upload_bytes(
            sql_dump.encode(),
            backup_path,
            bucket=self.backup_bucket
        )
        
        return backup_path
    
    def _backup_artifacts(self, backup_id: str) -> List[str]:
        """Backup all model artifacts."""
        backup_paths = []
        
        with get_db_session() as session:
            models = session.query(ModelRegistry).filter(
                ModelRegistry.is_active == True
            ).all()
            
            for model in models:
                # Copy model artifact
                if model.artifact_uri:
                    backup_path = f"backups/{backup_id}/artifacts/{model.id}_model.joblib.zst"
                    self.artifact_store.copy_object(
                        model.artifact_uri,
                        backup_path,
                        target_bucket=self.backup_bucket
                    )
                    backup_paths.append(backup_path)
                
                # Copy explanations if exists
                if model.explanation_uri:
                    backup_path = f"backups/{backup_id}/artifacts/{model.id}_explanations.joblib.zst"
                    self.artifact_store.copy_object(
                        model.explanation_uri,
                        backup_path,
                        target_bucket=self.backup_bucket
                    )
                    backup_paths.append(backup_path)
        
        return backup_paths
    
    def _export_database(self) -> str:
        """Export database schema and data."""
        # This is a placeholder - actual implementation would depend on database type
        return "-- Database export placeholder"
    
    def _calculate_backup_size(self, db_path: str, artifact_paths: List[str]) -> float:
        """Calculate total backup size in MB."""
        total_size = 0
        
        try:
            # Get database backup size
            db_size = self.artifact_store.get_object_size(db_path, bucket=self.backup_bucket)
            total_size += db_size
            
            # Get artifact sizes
            for path in artifact_paths:
                size = self.artifact_store.get_object_size(path, bucket=self.backup_bucket)
                total_size += size
        except Exception as e:
            logger.warning(f"Failed to calculate backup size: {e}")
        
        return total_size / (1024 * 1024)  # Convert to MB
    
    def _replicate_to_cross_region(self, backup_id: str, manifest: Dict[str, Any]):
        """Replicate backup to cross-region bucket."""
        try:
            # Copy manifest
            manifest_path = f"backups/{backup_id}/manifest.json"
            self.artifact_store.copy_cross_region(
                manifest_path,
                self.backup_bucket,
                self.cross_region_bucket
            )
            
            # Copy database backup
            self.artifact_store.copy_cross_region(
                manifest['database_backup'],
                self.backup_bucket,
                self.cross_region_bucket
            )
            
            # Copy artifacts
            for artifact_path in manifest['artifact_backups']:
                self.artifact_store.copy_cross_region(
                    artifact_path,
                    self.backup_bucket,
                    self.cross_region_bucket
                )
            
            logger.info(f"Cross-region replication completed for backup: {backup_id}")
        except Exception as e:
            logger.warning(f"Cross-region replication failed: {e}")

    def _restore_database(self, backup_path: str, bucket: str):
        """Restore database from backup."""
        try:
            # Download database backup
            sql_data = self.artifact_store.download_bytes(backup_path, bucket=bucket)
            sql_dump = sql_data.decode()
            
            # Execute SQL to restore database
            with get_db_session() as session:
                session.execute(text(sql_dump))
                session.commit()
            
            logger.info(f"Database restored from: {backup_path}")
        except Exception as e:
            logger.error(f"Database restore failed: {e}")
            raise

    def _restore_artifacts(self, artifact_paths: List[str], bucket: str):
        """Restore artifacts from backup."""
        for path in artifact_paths:
            try:
                # Extract model ID from path
                filename = path.split('/')[-1]
                model_id = filename.split('_')[0]
                
                # Determine target path based on artifact type
                if 'model.joblib' in filename:
                    target_path = f"models/{model_id}/model.joblib"
                elif 'explanations.joblib' in filename:
                    target_path = f"models/{model_id}/explanations.joblib"
                else:
                    continue
                
                # Copy artifact to original location
                self.artifact_store.copy_object(
                    path,
                    target_path,
                    source_bucket=bucket
                )
                
            except Exception as e:
                logger.warning(f"Failed to restore artifact {path}: {e}")

# Initialize disaster recovery manager
try:
    from app.storage.artifact_store import get_default_artifact_store
    artifact_store = get_default_artifact_store()
    disaster_recovery_manager = DisasterRecoveryManager(artifact_store)
    logger.info("Disaster recovery manager initialized")
except Exception as e:
    logger.warning(f"Failed to initialize disaster recovery manager: {e}")
    disaster_recovery_manager = None

logger.info(" Model Registry Enterprise Service with Disaster Recovery - 100% Complete!")
